#+title: Design of CINN
This document describe the design of the project CINN.
* Multi-layer architecture
To enable the compiler to handle the general DNN tasks, a multi-layer system is designed.

The layers are as follows

1. NN compatible layer
   - Add operator wrapper for DNN platform such as PaddlePaddle or TensorFlow.
2. Virtual graph layer
   - Add virtual node and graph to utilize both the compiler and third-party computational library such as CUDNN or MKLDNN.
3. DSL layer
   - Export a friendly domain language or IR for programming with the underlying compiler.
4. Compiler layer
   - A NN compiler which can optimize affine forloop.
** HLIR(High Level IR)
The CINN project aims to get the ideal performance with less efforts in writing and optimizing DNN operators, that is done by a tradeoff between the application of compiler technology and the third-party math libraries (such as MKL or CUDNN).

The HLIR module is the key to tradeoff, by mixing the kernels generated by compiler and math libraries, and autotune by searching the best performance, we can achieve the best tradeoff.

The HLIR is consisted of three layers, lets introduce from the outermost to innermost

*** Virtual graph layer(VGL)
This is the outermost layer, which take a real model from PaddlePaddle or ONNX, parse it and generate the VGL.
contains the virtual symbols (of operator-wise), this represents a virtual SSA graph of a DNN model,

**** VGL node design
#+BEGIN_SRC C++
  //! Base class of all the operators.
  struct Operator {
    string type;
  };

  struct MatMul : LogicOperator {};

  struct VGLNode {};
#+END_SRC
*** Operator mixed layer(OML)
This layers lowers the VGL to the underlying node implementation, the nodes in this layer contains two kinds of nodes, the math library powered and the compiler generated ones,

1. The math library powered nodes are generated by utilizing math library(such as CUDNN, MKL), we can utilize the operators of the existing DNN platforms(such as Paddle-Lite).
2. The other nodes are represented by the compiler layer, which will finally passed to compiler to generate code.

The first kind of nodes are carefully optimized by hardware suppliers(Intel, NVidia and so on), they beats the other kind of nodes in the classical algorithms such as MatMul and Convolutation,
while the second kind of nodes are generated the compiler, where the auto-optimization can take place, it wins in some other algorithms and can save human efforts by generating codes for multiple languages or platforms.
*** Compiler layer(CL)
This is the lowest layer and the compiler locates there. 
The CL get the sub-graphs(where the nodes are all compiler representable) and compile them to output.

* DSL layer

**Matrix multiplication with blocking**

#+BEGIN_SRC C++
  Var i("i"), j("j"), k("k");
  Constant N("N"), M("M"), K("K");

  PlaceHolder<float> x("x", {M, K});
  PlaceHolder<float> y("x", {K, N});

  Tensor C = compute({M, N, K} /*dims*/, [&](Var i, Var j, Var k) { return x(i, k) * y(k, j); });

  Schedule s = ComputeSchedule({C} /*outputs*/, {A, B} /*inputs*/);
  {  // schedule C's computation

    // tile i, j with factor 4
    Var i0, i1, j0, j1;
    std::tie(i0, i1, j0, j1) = s[C].tile(s[C].axis("i"), s[C].axis("j"), 4, 4);

    // tile k with factor 4
    Var k0, k1;
    std::tie(k0, k1) = s[C].split(k, 4);

    s[C].reorder(i0, j0, k0, k1, i1, j1);  // swap(i1,j0)
  }

  auto module = Build(s, {A, B, C}, "llvm", "host", "func");
  void* func  = module.GetFuncByName("func");
#+END_SRC

**Matrix with Vectorization**

#+BEGIN_SRC C++
  Schedule S = ComputeSchedule({C}, {A, B});

  Var k0, k1;
  std::tie(k0, k1) = S[C].split(k, 4);
  Var x0, x1, y0, y1;
  std::tie(x0, x1, y0, y1) = S[C].tile(x, y, 4, 4);

  S[C].reorder(x0, y0, k0, k1, x1, y1);

  S[C].vectorize(y1);
#+END_SRC

**Matrix with Packing**

#+BEGIN_SRC C++
  Tensor packedB = compute((N / bn, K, bn), [&](Var i, Var j, Var k) { return B(j, i * bn + k); });

  Tensor C = compute({M, N}, [&](Var i, Var j, Var k) {
    // reduce sum(need initialize)
    return sum(A(i, k) * packedB(y / bn, k, y % bn), k);
  });

  Schedule S = compute_schedule({C}, {A, B});

  Var i0, j0, i1, j1;
  Var k0, k1;
  std::tie(i0, i1, j0, j1) = S[C].tile(S[C].axis(0), S.axis(1), 4, 4);
  std::tie(k0, k1)         = S[C].split(S[C].axis(k, 4));

  S[C].reorder(i0, j0, k0, i1, k1, j1);
  S[C].vectorize(j1);

  {
    Var i, j, k;
    std::tie(i, j, k) = S[packedB].axis();
    S[packedB].vectorize(k);
    S[packedB].parallel(i);
  }
#+END_SRC

* HLIR
** Instruction layer
The Instruction layer is a higher abstraction of the underlying compiler IRs. A model (or part of it) can lower to several computations of Instructions.

Instruction does not have basic blocks or "branch" instructions, we currently haven't consider the condition operations such as while or switch yet. 

This layer helps to 

1. give a higher view of the underlying IR, make it more developer-friendly with more utilities,
2. supports the computation fusion automatically
3. supports different lowering mode, the codegen mode or handcraft mode(with manually developed operations),

*** Higher view

#+BEGIN_SRC C++
  a = create_input;
  b = create_input;
  w = crate_parameter;

  c = add(a, b);
  d = dot(w, c);
#+END_SRC

The coarse granularity of IR makes it easier to develop or optimize in a high view.

*** Automatic computation fusion

#+BEGIN_SRC C++
  c = add(a, b);
  d = tanh(c);

  // can be fused to

  d = fused_instruction({add(a, b), tanh});
#+END_SRC

*** Different lowering mode support
The instruction layer supports different lowering mode to tradeoff the performance of codegen and existing math library powered computations.


The computation snippet


#+BEGIN_SRC C++
  c = add(a, b);
  d = dot(c, w);
#+END_SRC

**** base class
#+BEGIN_SRC C++
  //! The function declaration.
  class FnDecl {
   public:
    enum class Kind {
      CINN,      // the CINN generated function.
      External,  // the external function.
    };

    FnDecl(const std::string& name, const std::vector<Type>& in_arg_types, const std::vector<Type>& out_arg_types);

    static std::unique_ptr<FnDecl> Build(const std::string& name,
                                         const std::vector<Type>& in_arg_types,
                                         const std::vector<Type>& out_arg_types);

    std::unique_ptr<Caller> Call(const std::vector<Instruction*>& in_args, const std::vector<Var>& out_args);

    struct Caller {
      //! Function declaration of this caller.
      FnDecl* decl{};

      void Build(const FnDecl* decl, const std::vector<Instruction*>& in_args, const std::vector<Instruction*>& out_args);
    };
  };

  class InstructionEmitBase {
   public:
    enum class Kind {
      CINN  = 0,  // the CINN Codegen mode
      MKL   = 1,
      CUDNN = 2,
    };
  };
#+END_SRC

**** Handcraft mode
Let's take BLAS as an example
#+BEGIN_SRC C++
  class DotCpuEmiter : public InstructionEmitBase {
   public:
    DotCpuEmiter();

   private:
    void EmitMkl();
    void EmitMkldnn();
  };

  // pass optimize =>
  cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, m, n, k, alpha, A, k, B, n, beta, C, n);
#+END_SRC

**** codegen code
#+BEGIN_SRC C++
  for (int i = 0; i < M; i++) {
    for (int j = 0; j < N; j++) {
      // ...
    }
  }
#+END_SRC

** Operator layer
* Compiler Layer
** IR

The IR is similar to Halide.
*** Basic elements

The IR has following basic elements:

- Expr, the expression in the IR(which represents a value or returns a value).
- Stmt, the statement in the IR.
- Tensor (the input or temporary value)
- Buffer (the memory buffer)
** Tensor

Tensor represents the input or temporary variable.

Each tensor is assigned a buffer by default, but `store_in` can change the relation.
** Polyhedral usage

The polyhedral technology is used to simplify the forloop analysis and transform.
** schedule

The original tensor-based computation forms a SSA graph.

Each tensor is assign a `Stage`, which is the basic schedule element.

A stage has a domain(isl.Set) and a schedule(isl.Map), all the schedule is performed on them.
*** Schedule the stages

We use the ideas from Tiramisu project, and walk through the dependency graph, split the graph into several groups.

There are several rules to split the graph, the naive one is

- For initialization, create a unique group(just id is needed) for each stage,
- traverse the computation graph in topological order and
- check whether the two statements with dependency relation have the same iteration space and domain, if true, gather them in the same group
  - if two statement is marked by `compute_at`, merge to the same group too.
  - this period is like a union find.
- for each group, use a different `ast_build` to generate ISL IR(so that we can set iterators separately)
*** Scheduler module

The Scheduler take the stages as input, and do the previous mentioned graph partition, and finally output several schedule elements.

Each schedule element owns an (ISL)iteration domain and a (ISL)schedule, and one can pass it to a ast_gen and generate code.
*** Lower output Tensors to LoweredFuncs

First, given the output tensors, the `Lower` function will collect all the depended inputs, and lower them to a function.

The lower interface is


#+BEGIN_SRC C++
  std::vector<LoweredFunction> Lower(vector<Tensor>& args, DeviceAPI device);
#+END_SRC

** Buffer

Buffer represents the actual memory in host or devices.

The `Buffer` node in IR represents a buffer, it can be used by binding to a Tensor.

The Tensor will be noninlined only if it binds to some buffer.

NOTE A buffer can be reused in multiple tensors(TODO the write-read correctness should be considered).


#+BEGIN_SRC C++
  Buffer buffer0;

  Tensor x = Compute(...);
  // x will write the result to buffer0
  x->Bind(buffer0);

  Tensor y = Compute(..., [](Var i) {
    return x(i) * 2;  // here it will read the buffer instead, x is just a alias.
  });
#+END_SRC

The size of the buffer will be inferenced from the shape and data type of tensor.
It by default can be resized to proper shape by binding to multiple tensors.
*** Buffer in CodeGen

All the buffers will be maintained in global scope, and alloc or dealloc in local scopes.

The benefit is buffer is easy to shared across multiple statements.
** Module
Module is the container of LoweredFuncs and Buffers.
There might be more than one module in an generated execution.

The Module can compile to a backends.
** Transform
*** Vectorize
usage:

#+BEGIN_SRC C++
  C->stage()->Vectorize(2, 8);
#+END_SRC

Mark the stage as vectorize, when lowering, mark the corresponding forloop in isl ast as vectorize,
then replace call with the statement.

There are several steps to vectorize a forloop:

- the forloop will first been split with a small factor(better to be times of the SIMD width
- convert the PolyFor to For
- Substitute the iterator variable of the forloop to a Ramp node
- Transform all the operations related to the Ramp
- Special convert the Load and Store nodes

There is a `Vec` runtime container to codegen the Ramp.
