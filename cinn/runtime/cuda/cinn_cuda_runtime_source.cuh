/**
 * \file This file contains all the intrinsics available to be used in CUDA code generated by CodeGen.
 */

#define FN(x) cinn_nvgpu_##x##_fp32
// NOTE Due to function override, we don't need to use type (such as '_fp32') as the suffix of function's name.
__device__ inline float FN(sin)(float x) { return sin(x); }
__device__ inline float FN(cos)(float x) { return cos(x); }
__device__ inline float FN(cosh)(float x) { return cosh(x); }
__device__ inline float FN(tanh)(float x) { return tanh(x); }

__device__ inline float FN(asin)(float x) { return asin(x); }
__device__ inline float FN(acos)(float x) { return acos(x); }
__device__ inline float FN(acosh)(float x) { return acosh(x); }
__device__ inline float FN(atanh)(float x) { return atanh(x); }

__device__ inline float FN(ceil)(float x) { return ceil(x); }
__device__ inline float FN(round)(float x) { return round(x); }
__device__ inline float FN(trunc)(float x) { return trunc(x); }
__device__ inline float FN(abs)(float x) { return abs(x); }
__device__ inline float FN(floor)(float x) { return floor(x); }
__device__ inline float FN(log)(float x) { return log(x); }
__device__ inline float FN(log2)(float x) { return log2(x); }
__device__ inline float FN(log10)(float x) { return log10(x); }
__device__ inline float FN(exp)(float x) { return exp(x); }
__device__ inline float FN(erf)(float x) { return erf(x); }
__device__ inline float FN(sigmoid)(float x) { return 1. / (1 + exp(-x)); }
__device__ inline float FN(sqrt)(float x) { return sqrt(x); }
__device__ inline float FN(rsqrt)(float x) { return rsqrt(x); }

__device__ inline bool FN(isfinite)(float x) { return isfinite(x); }
__device__ inline bool FN(isinf)(float x) { return isinf(x); }
__device__ inline bool FN(isnan)(float x) { return isnan(x); }

__device__ inline float FN(max)(float a, float b) { return max(a, b); }
__device__ inline float FN(min)(float a, float b) { return min(a, b); }

#undef FN

template <class Dtype>
__device__ inline Dtype cinn_max(const Dtype left, const Dtype right) {
  return max(left, right);
}

template <class Dtype>
__device__ inline Dtype cinn_min(const Dtype left, const Dtype right) {
  return min(left, right);
}

template <class Dtype>
__device__ inline Dtype cinn_sum(const Dtype left, const Dtype right) {
  return left + right;
}

template <class Dtype>
__device__ inline Dtype cinn_prod(const Dtype left, const Dtype right) {
  return left * right;
}

template <class Dtype, Dtype (*Operator)(const Dtype, const Dtype)>
__device__ inline Dtype cinn_warp_shuffle_internal(const Dtype value) {
  Dtype tmp         = value;
  unsigned int mask = __activemask();
  // do shuffle
  tmp = Operator(tmp, __shfl_down_sync(mask, tmp, 16, 32));
  tmp = Operator(tmp, __shfl_down_sync(mask, tmp, 8, 32));
  tmp = Operator(tmp, __shfl_down_sync(mask, tmp, 4, 32));
  tmp = Operator(tmp, __shfl_down_sync(mask, tmp, 2, 32));
  tmp = Operator(tmp, __shfl_down_sync(mask, tmp, 1, 32));
  // broadcast to all threads
  tmp = __shfl_sync(mask, tmp, 0, 32);
  return tmp;
}

__device__ inline float cinn_warp_reduce_max(const float *buf, int offset, int extend) {
  float tmp_val = -3.402823e+38f;
  for (int i = threadIdx.x; i < extend; i += 32) {
    tmp_val = max(tmp_val, buf[offset + i]);
  }
  return cinn_warp_shuffle_internal<float, cinn_max<float>>(tmp_val);
}

__device__ inline float cinn_warp_reduce_sum(const float *buf, int offset, int extend) {
  float tmp_val = 0.0f;
  for (int i = threadIdx.x; i < extend; i += 32) {
    tmp_val += buf[offset + i];
  }
  return cinn_warp_shuffle_internal<float, cinn_sum<float>>(tmp_val);
}

__device__ inline float cinn_warp_reduce_avg(const float *buf, int offset, int extend) {
  return cinn_warp_reduce_sum(buf, offset, extend) / extend;
}

template <class Dtype, Dtype (*Operator)(const Dtype, const Dtype)>
__device__ inline Dtype cinn_block_reduce_internal(const Dtype value) {
  int warp_id = threadIdx.x / 32;
  __shared__ Dtype tmp[32];
  if (warp_id == 0) {
    tmp[threadIdx.x] = 0.0f;
  }
  Dtype tmp_val = cinn_warp_shuffle_internal<Dtype, Operator>(value);
  if (blockDim.x <= 32) {
    return tmp_val;
  }
  __syncthreads();
  if (threadIdx.x % 32 == 0) {
    tmp[warp_id] = tmp_val;
  }
  __syncthreads();
  if (warp_id == 0) {
    tmp_val = tmp[threadIdx.x];
    tmp_val = cinn_warp_shuffle_internal<Dtype, Operator>(tmp_val);
    if (threadIdx.x == 0) {
      tmp[0] = tmp_val;
    }
  }
  __syncthreads();
  return tmp[0];
}

auto cinn_block_reduce_max_internal  = cinn_block_reduce_internal<float, cinn_max<float>>;
auto cinn_block_reduce_min_internal  = cinn_block_reduce_internal<float, cinn_min<float>>;
auto cinn_block_reduce_sum_internal  = cinn_block_reduce_internal<float, cinn_sum<float>>;
auto cinn_block_reduce_prod_internal = cinn_block_reduce_internal<float, cinn_prod<float>>;

template <class Dtype, Dtype (*Operator)(const Dtype, const Dtype)>
__device__ inline float cinn_block_reduce(const Dtype *buf, int offset, int extend) {
  Dtype tmp_val = 0;
  for (int i = threadIdx.x; i < extend; i += blockDim.x) {
    tmp_val = Operator(tmp_val, buf[offset + i]);
  }

  return cinn_block_reduce_internal<Dtype, Operator>(tmp_val);
}

auto cinn_block_reduce_max  = cinn_block_reduce<float, cinn_max<float>>;
auto cinn_block_reduce_min  = cinn_block_reduce<float, cinn_min<float>>;
auto cinn_block_reduce_sum  = cinn_block_reduce<float, cinn_sum<float>>;
auto cinn_block_reduce_prod = cinn_block_reduce<float, cinn_prod<float>>;
